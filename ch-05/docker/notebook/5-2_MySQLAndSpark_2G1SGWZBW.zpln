{
  "paragraphs": [
    {
      "text": "%md\n# MySQL DataSource via JDBC DataFrameReader\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 00:43:49.039",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eMySQL DataSource via JDBC DataFrameReader\u003c/h1\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614201359165_586278564",
      "id": "paragraph_1614201359165_586278564",
      "dateCreated": "2021-02-24 21:15:59.165",
      "dateStarted": "2021-03-08 00:43:49.050",
      "dateFinished": "2021-03-08 00:43:49.061",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n// shared configuration\n// note: this password is for Docker mysql only - it isn\u0027t smart to add passwords to code that is version controlled!\nval jdbcDriver \u003d spark.conf.get(\"spark.jdbc.driver.class\", \"org.mariadb.jdbc.Driver\")\nval dbHost     \u003d spark.conf.get(\"spark.jdbc.host\",\"mysql\")\nval dbPort     \u003d spark.conf.get(\"spark.jdbc.port\", \"3306\")\nval defaultDb  \u003d spark.conf.get(\"spark.jdbc.default.db\", \"default\")\nval dbTable    \u003d spark.conf.get(\"spark.jdbc.table\", \"customers\")\nval dbUser     \u003d spark.conf.get(\"spark.jdbc.user\", \"dataeng\")\nval dbPass     \u003d spark.conf.get(\"spark.jdbc.password\", \"dataengineering_user\")\n\nval connectionUrl \u003d s\"jdbc:mysql://$dbHost:$dbPort/$defaultDb\"\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 16:59:22.536",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mjdbcDriver\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d org.mariadb.jdbc.Driver\n\u001b[1m\u001b[34mdbHost\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d mysql\n\u001b[1m\u001b[34mdbPort\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d 3306\n\u001b[1m\u001b[34mdefaultDb\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d default\n\u001b[1m\u001b[34mdbTable\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d customers\n\u001b[1m\u001b[34mdbUser\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d dataeng\n\u001b[1m\u001b[34mdbPass\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d dataengineering_user\n\u001b[1m\u001b[34mconnectionUrl\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d jdbc:mysql://mysql:3306/default\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614213590359_2024560442",
      "id": "paragraph_1614213590359_2024560442",
      "dateCreated": "2021-02-25 00:39:50.359",
      "dateStarted": "2021-03-08 16:59:22.557",
      "dateFinished": "2021-03-08 16:59:24.866",
      "status": "FINISHED"
    },
    {
      "title": "Reading Data From JDBC Connections",
      "text": "%spark\nval customers \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", connectionUrl)\n  .option(\"driver\", jdbcDriver)\n  .option(\"dbtable\", dbTable)\n  .option(\"user\", dbUser)\n  .option(\"password\", dbPass)\n  .load()\n\ncustomers.printSchema\ncustomers.createOrReplaceTempView(\"customers\")",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 16:59:27.868",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- id: string (nullable \u003d true)\n |-- created: timestamp (nullable \u003d true)\n |-- updated: timestamp (nullable \u003d true)\n |-- first_name: string (nullable \u003d true)\n |-- last_name: string (nullable \u003d true)\n |-- email: string (nullable \u003d true)\n\n\u001b[1m\u001b[34mcustomers\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: string, created: timestamp ... 4 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614184981488_759162661",
      "id": "paragraph_1614184981488_759162661",
      "dateCreated": "2021-02-24 16:43:01.488",
      "dateStarted": "2021-03-08 16:59:27.895",
      "dateFinished": "2021-03-08 16:59:31.122",
      "status": "FINISHED"
    },
    {
      "title": "Write New Customers to MySQL",
      "text": "%spark\nimport org.apache.spark.sql._\nimport java.sql.Timestamp\nimport java.time._\n\n// spark: SparkSession (from spark-shell)\nassert(spark.isInstanceOf[SparkSession])\n\ndef ts(timeStr: String) \u003d Timestamp.valueOf(timeStr)\ndef time \u003d Timestamp.from(Instant.now())\n\n// create some new customers\nval records \u003d Seq(\n  Row(\"4\",ts(\"2021-02-21 21:00:00\"),time,\"Penny\",\"Haines\",\"penny@coffeeco.com\"),\n  Row(\"5\",ts(\"2021-02-21 22:00:00\"),time,\"Cloud\",\"Fast\",\"cloud.fast@acme.com\"),\n  Row(\"6\",ts(\"2021-02-21 23:00:00\"),time,\"Marshal\",\"Haines\",\"paws@coffeeco.com\")\n)\n\n// generate a new DataFrame using the new records and the schema from df (connect-jdbc.scala)\nval newCustomers \u003d spark.createDataFrame(\n    spark.sparkContext.parallelize(records),\n    customers.schema\n)\n\n// note: doesn\u0027t deduplicate if set to `mode(\"ignore\")`. It will just do nothing.\nnewCustomers\n  .write\n  .format(\"jdbc\")\n  //.mode(\"append\") // change to ignore to stop adding duplicates\n  .options(Map[String, String](\n    \"url\" -\u003e connectionUrl,\n    \"driver\" -\u003e jdbcDriver,\n    \"dbtable\" -\u003e dbTable,\n    \"user\" -\u003e dbUser,\n    \"password\" -\u003e dbPass\n  )\n).save()\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:08:53.638",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.AnalysisException: Table or view \u0027customers\u0027 already exists. SaveMode: ErrorIfExists.\n  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n  ... 52 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614213413596_418016277",
      "id": "paragraph_1614213413596_418016277",
      "dateCreated": "2021-02-25 00:36:53.596",
      "dateStarted": "2021-03-08 17:08:41.736",
      "dateFinished": "2021-03-08 17:08:42.559",
      "status": "ERROR"
    },
    {
      "text": "%sql\nselect * from customers",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 16:59:33.371",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "created": "string",
                      "updated": "string",
                      "first_name": "string",
                      "last_name": "string",
                      "email": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "id\tcreated\tupdated\tfirst_name\tlast_name\temail\n1\t2021-03-08 00:48:42.0\t2021-03-08 00:48:42.0\tScott\tHaines\tscott@coffeeco.com\n2\t2021-03-08 00:48:42.0\t2021-03-08 00:48:42.0\tJohn\tHamm\tjohn.hamm@acme.com\n3\t2021-03-08 00:48:42.0\t2021-03-08 00:48:42.0\tMilo\tHaines\tmhaines@coffeeco.com\n6\t2021-02-21 23:00:00.0\t2021-02-21 23:00:00.0\tMarshal\tHaines\tpaws@coffeeco.com\n4\t2021-02-21 21:00:00.0\t2021-02-21 21:00:00.0\tPenny\tHaines\tpenny@coffeeco.com\n5\t2021-02-21 22:00:00.0\t2021-02-21 22:00:00.0\tCloud\tFast\tcloud.fast@acme.com\n5\t2021-02-21 22:00:00.0\t2021-02-21 22:00:00.0\tCloud\tFast\tcloud.fast@acme.com\n6\t2021-02-21 23:00:00.0\t2021-02-21 23:00:00.0\tMarshal\tHaines\tpaws@coffeeco.com\n4\t2021-02-21 21:00:00.0\t2021-02-21 21:00:00.0\tPenny\tHaines\tpenny@coffeeco.com\n4\t2021-02-21 21:00:00.0\tnull\tPenny\tHaines\tpenny@coffeeco.com\n5\t2021-02-21 22:00:00.0\tnull\tCloud\tFast\tcloud.fast@acme.com\n6\t2021-02-21 23:00:00.0\tnull\tMarshal\tHaines\tpaws@coffeeco.com\n4\t2021-02-21 21:00:00.0\t2021-03-08 01:37:35.0\tPenny\tHaines\tpenny@coffeeco.com\n5\t2021-02-21 22:00:00.0\t2021-03-08 01:37:35.0\tCloud\tFast\tcloud.fast@acme.com\n6\t2021-02-21 23:00:00.0\t2021-03-08 01:37:35.0\tMarshal\tHaines\tpaws@coffeeco.com\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d0"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614213752264_1780383982",
      "id": "paragraph_1614213752264_1780383982",
      "dateCreated": "2021-02-25 00:42:32.264",
      "dateStarted": "2021-03-08 16:59:33.394",
      "dateFinished": "2021-03-08 16:59:36.746",
      "status": "FINISHED"
    },
    {
      "title": "Get all the distinct emails",
      "text": "%sql\nselect distinct(email) as email from customers",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 01:02:35.031",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "email": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/sql",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "email\nscott@coffeeco.com\npenny@coffeeco.com\npaws@coffeeco.com\nmhaines@coffeeco.com\ncloud.fast@acme.com\njohn.hamm@acme.com\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d10"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d11"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d12"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d13"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d14"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615165203249_337912082",
      "id": "paragraph_1615165203249_337912082",
      "dateCreated": "2021-03-08 01:00:03.249",
      "dateStarted": "2021-03-08 01:01:28.715",
      "dateFinished": "2021-03-08 01:01:29.898",
      "status": "FINISHED"
    },
    {
      "title": "Select Distinct and Sort",
      "text": "%sql\nSELECT id, min(created) as created, max(updated) as updated, email FROM customers\nGROUP BY id, email\nORDER BY id ASC",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 01:55:02.843",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "created": "string",
                      "updated": "string",
                      "email": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/sql",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "id\tcreated\tupdated\temail\n1\t2021-03-08 00:48:42.0\t2021-03-08 00:48:42.0\tscott@coffeeco.com\n2\t2021-03-08 00:48:42.0\t2021-03-08 00:48:42.0\tjohn.hamm@acme.com\n3\t2021-03-08 00:48:42.0\t2021-03-08 00:48:42.0\tmhaines@coffeeco.com\n4\t2021-02-21 21:00:00.0\t2021-03-08 01:37:35.0\tpenny@coffeeco.com\n5\t2021-02-21 22:00:00.0\t2021-03-08 01:37:35.0\tcloud.fast@acme.com\n6\t2021-02-21 23:00:00.0\t2021-03-08 01:37:35.0\tpaws@coffeeco.com\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d59"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615165041437_790866939",
      "id": "paragraph_1615165041437_790866939",
      "dateCreated": "2021-03-08 00:57:21.437",
      "dateStarted": "2021-03-08 01:50:26.500",
      "dateFinished": "2021-03-08 01:50:27.064",
      "status": "FINISHED"
    },
    {
      "text": "%md\n# Fixing the Customers Table\n\n~~~sql\nCREATE TABLE IF NOT EXISTS `customers` (\n  `id` text,\n  `created` timestamp NULL DEFAULT NULL,\n  `updated` timestamp NULL DEFAULT NULL,\n  `first_name` text,\n  `last_name` text,\n  `email` text\n);\n~~~\n\nThe initial customers table we whipped up in Exercise 5-1 is a little naive. We can do *better*.\n\n## Creating a Better Customers Table\nThe `bettercustomers` table has to be created first inside the database because spark doesn\u0027t recognize `AUTO_INCREMENT` in the table definition.\n~~~sql\nCREATE TABLE IF NOT EXISTS `bettercustomers` (\n  `id` mediumint NOT NULL AUTO_INCREMENT COMMENT \u0027customer automatic id\u0027,\n  `created` timestamp DEFAULT CURRENT_TIMESTAMP COMMENT \u0027customer join date\u0027,\n  `updated` timestamp DEFAULT CURRENT_TIMESTAMP COMMENT \u0027last record update\u0027,\n  `first_name` varchar(100) NOT NULL,\n  `last_name` varchar(100) NOT NULL,\n  `email` varchar(255) NOT NULL UNIQUE,\n  PRIMARY KEY (`id`)\n);\n~~~\n\n### Inserting Customer Records Into BetterCustomers Table\n~~~sql\nINSERT INTO bettercustomers (created, first_name, last_name, email)\nVALUES\n(\"2021-02-16 00:16:06\", \"Scott\", \"Haines\", \"scott@coffeeco.com\"),\n(\"2021-02-16 00:16:06\", \"John\", \"Hamm\", \"john.hamm@acme.com\"),\n(\"2021-02-16 00:16:06\", \"Milo\", \"Haines\", \"mhaines@coffeeco.com\"),\n(\"2021-02-21 21:00:00\", \"Penny\", \"Haines\", \"penny@coffeeco.com\"),\n(\"2021-02-21 22:00:00\", \"Cloud\", \"Fast\", \"cloud.fast@acme.com\"),\n(\"2021-02-21 23:00:00\", \"Marshal\", \"Haines\", \"paws@coffeeco.com\"),\n(\"2021-02-24 09:00:00\", \"Willow\", \"Haines\", \"willow@coffeeco.com\"),\n(\"2021-02-24 09:00:00\", \"Clover\", \"Haines\", \"pup@coffeeco.com\");\n~~~",
      "user": "anonymous",
      "dateUpdated": "2021-03-05 22:39:18.714",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eFixing the Customers Table\u003c/h1\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sql\"\u003eCREATE TABLE IF NOT EXISTS `customers` (\n  `id` text,\n  `created` timestamp NULL DEFAULT NULL,\n  `updated` timestamp NULL DEFAULT NULL,\n  `first_name` text,\n  `last_name` text,\n  `email` text\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe initial customers table we whipped up in Exercise 5-1 is a little naive. We can do \u003cem\u003ebetter\u003c/em\u003e.\u003c/p\u003e\n\u003ch2\u003eCreating a Better Customers Table\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003ebettercustomers\u003c/code\u003e table has to be created first inside the database because spark doesn\u0026rsquo;t recognize \u003ccode\u003eAUTO_INCREMENT\u003c/code\u003e in the table definition.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sql\"\u003eCREATE TABLE IF NOT EXISTS `bettercustomers` (\n  `id` mediumint NOT NULL AUTO_INCREMENT COMMENT \u0027customer automatic id\u0027,\n  `created` timestamp DEFAULT CURRENT_TIMESTAMP COMMENT \u0027customer join date\u0027,\n  `updated` timestamp DEFAULT CURRENT_TIMESTAMP COMMENT \u0027last record update\u0027,\n  `first_name` varchar(100) NOT NULL,\n  `last_name` varchar(100) NOT NULL,\n  `email` varchar(255) NOT NULL UNIQUE,\n  PRIMARY KEY (`id`)\n);\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eInserting Customer Records Into BetterCustomers Table\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sql\"\u003eINSERT INTO bettercustomers (created, first_name, last_name, email)\nVALUES\n(\u0026quot;2021-02-16 00:16:06\u0026quot;, \u0026quot;Scott\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;scott@coffeeco.com\u0026quot;),\n(\u0026quot;2021-02-16 00:16:06\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;Hamm\u0026quot;, \u0026quot;john.hamm@acme.com\u0026quot;),\n(\u0026quot;2021-02-16 00:16:06\u0026quot;, \u0026quot;Milo\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;mhaines@coffeeco.com\u0026quot;),\n(\u0026quot;2021-02-21 21:00:00\u0026quot;, \u0026quot;Penny\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;penny@coffeeco.com\u0026quot;),\n(\u0026quot;2021-02-21 22:00:00\u0026quot;, \u0026quot;Cloud\u0026quot;, \u0026quot;Fast\u0026quot;, \u0026quot;cloud.fast@acme.com\u0026quot;),\n(\u0026quot;2021-02-21 23:00:00\u0026quot;, \u0026quot;Marshal\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;paws@coffeeco.com\u0026quot;),\n(\u0026quot;2021-02-24 09:00:00\u0026quot;, \u0026quot;Willow\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;willow@coffeeco.com\u0026quot;),\n(\u0026quot;2021-02-24 09:00:00\u0026quot;, \u0026quot;Clover\u0026quot;, \u0026quot;Haines\u0026quot;, \u0026quot;pup@coffeeco.com\u0026quot;);\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614213858686_160867183",
      "id": "paragraph_1614213858686_160867183",
      "dateCreated": "2021-02-25 00:44:18.686",
      "dateStarted": "2021-03-05 22:39:18.716",
      "dateFinished": "2021-03-05 22:39:18.731",
      "status": "FINISHED"
    },
    {
      "title": "Load up our Better Customers",
      "text": "%spark\nval betterCustomers \u003d spark.read\n  .format(\"jdbc\")\n  .option(\"url\", connectionUrl)\n  .option(\"driver\", jdbcDriver)\n  .option(\"dbtable\", \"bettercustomers\")\n  .option(\"user\", dbUser)\n  .option(\"password\", dbPass)\n  .load()\n\nbetterCustomers.printSchema\n\nbetterCustomers.show(false)\n\nbetterCustomers.createOrReplaceTempView(\"bettercustomers\")",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:23:54.204",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- id: integer (nullable \u003d true)\n |-- created: timestamp (nullable \u003d true)\n |-- updated: timestamp (nullable \u003d true)\n |-- first_name: string (nullable \u003d true)\n |-- last_name: string (nullable \u003d true)\n |-- email: string (nullable \u003d true)\n\n+---+-------------------+-------------------+----------+---------+--------------------+\n|id |created            |updated            |first_name|last_name|email               |\n+---+-------------------+-------------------+----------+---------+--------------------+\n|1  |2021-02-16 00:16:06|2021-03-08 17:23:41|Scott     |Haines   |scott@coffeeco.com  |\n|2  |2021-02-16 00:16:06|2021-03-08 17:23:41|John      |Hamm     |john.hamm@acme.com  |\n|3  |2021-02-16 00:16:06|2021-03-08 17:23:41|Milo      |Haines   |mhaines@coffeeco.com|\n|4  |2021-02-21 21:00:00|2021-03-08 17:23:41|Penny     |Haines   |penny@coffeeco.com  |\n|5  |2021-02-21 22:00:00|2021-03-08 17:23:41|Cloud     |Fast     |cloud.fast@acme.com |\n|6  |2021-02-21 23:00:00|2021-03-08 17:23:41|Marshal   |Haines   |paws@coffeeco.com   |\n|7  |2021-02-24 09:00:00|2021-03-08 17:23:41|Willow    |Haines   |willow@coffeeco.com |\n|8  |2021-02-24 09:00:00|2021-03-08 17:23:41|Clover    |Haines   |pup@coffeeco.com    |\n+---+-------------------+-------------------+----------+---------+--------------------+\n\n\u001b[1m\u001b[34mbetterCustomers\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: int, created: timestamp ... 4 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d3"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614185019787_1496963524",
      "id": "paragraph_1614185019787_1496963524",
      "dateCreated": "2021-02-24 16:43:39.787",
      "dateStarted": "2021-03-08 17:23:54.222",
      "dateFinished": "2021-03-08 17:23:54.838",
      "status": "FINISHED"
    },
    {
      "title": "Getting the Schema",
      "text": "%spark\nbetterCustomers.schema",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:23:59.313",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres12\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.types.StructType\u001b[0m \u003d StructType(StructField(id,IntegerType,true), StructField(created,TimestampType,true), StructField(updated,TimestampType,true), StructField(first_name,StringType,true), StructField(last_name,StringType,true), StructField(email,StringType,true))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614186266127_1397870333",
      "id": "paragraph_1614186266127_1397870333",
      "dateCreated": "2021-02-24 17:04:26.127",
      "dateStarted": "2021-03-08 17:23:59.341",
      "dateFinished": "2021-03-08 17:23:59.612",
      "status": "FINISHED"
    },
    {
      "title": "Write Better Customer Records with Minimal Schema",
      "text": "%spark\n\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\n\nval simpleFieldNames \u003d Set(\"first_name\", \"last_name\", \"email\")\nval simpleSchema \u003d StructType(\n    betterCustomers.schema.fields.filter(sf \u003d\u003e simpleFieldNames.contains(sf.name))\n)\n\nval saveMode \u003d \"append\" // change this to append to write through. The email constraint means this can only be run once - cause otherwise we\u0027d get an error.\n\n// create simplified Rows!\nval rows \u003d spark.createDataFrame(\n  spark.sparkContext.parallelize(\n      Seq(\n          Row(\"Nanna\",\"Haines\",\"nhugs@coffeeco.com\"),\n          Row(\"The\",\"Rock\",\"djohnson@coffeeco.com\")\n      )\n  ),\n  simpleSchema\n)\n\nrows\n.write\n.format(\"jdbc\")\n.mode(saveMode)\n.options(Map[String, String](\n  \"url\" -\u003e connectionUrl,\n  \"driver\" -\u003e jdbcDriver,\n  \"dbtable\" -\u003e \"bettercustomers\",\n  \"user\" -\u003e dbUser,\n  \"password\" -\u003e dbPass/*,\n  \"truncate\" -\u003e \"true\"*/\n))\n.save()",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:25:02.039",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 167.798,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 12) (zeppelin executor driver): java.sql.BatchUpdateException: (conn\u003d42) Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n\tat org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)\n\tat org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:692)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.SQLIntegrityConstraintViolationException: (conn\u003d42) Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n\tat org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:70)\n\tat org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)\n\tat org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)\n\t... 15 more\nCaused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n\tat org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)\n\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)\n\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)\n\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)\n\tat org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)\n\tat org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\t... 3 more\nCaused by: java.sql.SQLException: Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)\n\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)\n\tat org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)\n\tat org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)\n\t... 5 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n  at scala.Option.foreach(Option.scala:407)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n  at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:854)\n  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n  ... 60 elided\nCaused by: java.sql.BatchUpdateException: (conn\u003d42) Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n  at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:324)\n  at org.mariadb.jdbc.ClientSidePreparedStatement.executeBatch(ClientSidePreparedStatement.java:300)\n  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:692)\n  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)\n  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)\n  at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n  at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n  at org.apache.spark.scheduler.Task.run(Task.scala:131)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n  ... 3 more\nCaused by: java.sql.SQLIntegrityConstraintViolationException: (conn\u003d42) Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n  at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.createException(ExceptionFactory.java:70)\n  at org.mariadb.jdbc.internal.util.exceptions.ExceptionFactory.create(ExceptionFactory.java:153)\n  at org.mariadb.jdbc.MariaDbStatement.executeBatchExceptionEpilogue(MariaDbStatement.java:320)\n  ... 15 more\nCaused by: org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException: Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n  at org.mariadb.jdbc.internal.util.exceptions.MariaDbSqlException.of(MariaDbSqlException.java:34)\n  at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.exceptionWithQuery(AbstractQueryProtocol.java:192)\n  at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.access$000(AbstractQueryProtocol.java:106)\n  at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol$1.handleResultException(AbstractQueryProtocol.java:690)\n  at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:141)\n  at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:67)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n  ... 3 more\nCaused by: java.sql.SQLException: Duplicate entry \u0027nhugs@coffeeco.com\u0027 for key \u0027bettercustomers.email\u0027\n  at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readErrorPacket(AbstractQueryProtocol.java:1681)\n  at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.readPacket(AbstractQueryProtocol.java:1543)\n  at org.mariadb.jdbc.internal.protocol.AbstractQueryProtocol.getResult(AbstractQueryProtocol.java:1506)\n  at org.mariadb.jdbc.internal.protocol.AsyncMultiRead.call(AsyncMultiRead.java:132)\n  ... 5 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d5"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614224798281_1065230385",
      "id": "paragraph_1614224798281_1065230385",
      "dateCreated": "2021-02-25 03:46:38.282",
      "dateStarted": "2021-03-08 17:24:49.978",
      "dateFinished": "2021-03-08 17:24:51.097",
      "status": "ERROR"
    },
    {
      "text": "%md\n## Note\nIt is worth pointing out that if you use `mode(\"overwrite\")` in the **Write Customer Records without Supplying Automatic Fields** example above, then the Table schema will actually be rewritten which isn\u0027t certainly not what we want.\n\n### Why is that? \nGiven that Spark uses the DataFrame schema to define the format of the table, when we call `write.mode(\"overwrite\")...` we will actually be doing the following.\n1. First Spark will Drop the table (if it already exists)\n2. Second Spark will use the `rows.schema` to define the new create table syntax\n3. Lastly, Spark will write the rows into the database\n\nThis would change the table schema, and that is bad for business.\n\n### If the table is first `truncated` then it will preserve the original schema defined by you when you created the database table.\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:26:24.747",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eNote\u003c/h2\u003e\n\u003cp\u003eIt is worth pointing out that if you use \u003ccode\u003emode(\u0026quot;overwrite\u0026quot;)\u003c/code\u003e in the \u003cstrong\u003eWrite Customer Records without Supplying Automatic Fields\u003c/strong\u003e example above, then the Table schema will actually be rewritten which isn\u0026rsquo;t certainly not what we want.\u003c/p\u003e\n\u003ch3\u003eWhy is that?\u003c/h3\u003e\n\u003cp\u003eGiven that Spark uses the DataFrame schema to define the format of the table, when we call \u003ccode\u003ewrite.mode(\u0026quot;overwrite\u0026quot;)...\u003c/code\u003e we will actually be doing the following.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFirst Spark will Drop the table (if it already exists)\u003c/li\u003e\n\u003cli\u003eSecond Spark will use the \u003ccode\u003erows.schema\u003c/code\u003e to define the new create table syntax\u003c/li\u003e\n\u003cli\u003eLastly, Spark will write the rows into the database\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis would change the table schema, and that is bad for business.\u003c/p\u003e\n\u003ch3\u003eIf the table is first \u003ccode\u003etruncated\u003c/code\u003e then it will preserve the original schema defined by you when you created the database table.\u003c/h3\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614223864606_1489564927",
      "id": "paragraph_1614223864606_1489564927",
      "dateCreated": "2021-02-25 03:31:04.606",
      "dateStarted": "2021-03-08 17:26:24.747",
      "dateFinished": "2021-03-08 17:26:24.763",
      "status": "FINISHED"
    },
    {
      "title": "Safe Overwrite of the bettercustomers Table",
      "text": "%spark\n\n// use interpreted Spark SQL to generate a DataFrame from our bettercustomers table\nval stored \u003d spark.sql(\"select * from bettercustomers\")\nstored.cache // will persist the dataframe - but there is a catch, spark needs to pass over the data twice to cache it, and we can use the count or head trick to do just that\nstored.count // kicks off the cache\n\nstored\n  .write\n  .mode(\"overwrite\")\n  .format(\"jdbc\")\n  .option(\"url\", connectionUrl)\n  .option(\"driver\", jdbcDriver)\n  .option(\"dbtable\", \"bettercustomers\")\n  .option(\"user\", dbUser)\n  .option(\"password\", dbPass)\n  .option(\"truncate\", true)\n  .save()\n\n// we no longer need to store the cached backup\nstored.unpersist",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:26:37.703",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mstored\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: int, created: timestamp ... 4 more fields]\n\u001b[1m\u001b[34mres17\u001b[0m: \u001b[1m\u001b[32mstored.type\u001b[0m \u003d [id: int, created: timestamp ... 4 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d6"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d7"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614981430657_1900932859",
      "id": "paragraph_1614981430657_1900932859",
      "dateCreated": "2021-03-05 21:57:10.657",
      "dateStarted": "2021-03-08 17:26:37.717",
      "dateFinished": "2021-03-08 17:26:39.675",
      "status": "FINISHED"
    },
    {
      "text": "%sql\nselect * from bettercustomers;",
      "user": "anonymous",
      "dateUpdated": "2021-03-08 17:26:48.075",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "id": "string",
                      "created": "string",
                      "updated": "string",
                      "first_name": "string",
                      "last_name": "string",
                      "email": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "id\tcreated\tupdated\tfirst_name\tlast_name\temail\n1\t2021-02-16 00:16:06.0\t2021-03-08 17:23:41.0\tScott\tHaines\tscott@coffeeco.com\n2\t2021-02-16 00:16:06.0\t2021-03-08 17:23:41.0\tJohn\tHamm\tjohn.hamm@acme.com\n3\t2021-02-16 00:16:06.0\t2021-03-08 17:23:41.0\tMilo\tHaines\tmhaines@coffeeco.com\n4\t2021-02-21 21:00:00.0\t2021-03-08 17:23:41.0\tPenny\tHaines\tpenny@coffeeco.com\n5\t2021-02-21 22:00:00.0\t2021-03-08 17:23:41.0\tCloud\tFast\tcloud.fast@acme.com\n6\t2021-02-21 23:00:00.0\t2021-03-08 17:23:41.0\tMarshal\tHaines\tpaws@coffeeco.com\n7\t2021-02-24 09:00:00.0\t2021-03-08 17:23:41.0\tWillow\tHaines\twillow@coffeeco.com\n8\t2021-02-24 09:00:00.0\t2021-03-08 17:23:41.0\tClover\tHaines\tpup@coffeeco.com\n9\t2021-03-08 17:24:21.0\t2021-03-08 17:24:21.0\tThe\tRock\tdjohnson@coffeeco.com\n10\t2021-03-08 17:24:21.0\t2021-03-08 17:24:21.0\tNanna\tHaines\tnhugs@coffeeco.com\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d8"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614222679551_1672528284",
      "id": "paragraph_1614222679551_1672528284",
      "dateCreated": "2021-02-25 03:11:19.551",
      "dateStarted": "2021-03-08 17:26:48.090",
      "dateFinished": "2021-03-08 17:26:48.226",
      "status": "FINISHED"
    },
    {
      "text": "%sql\n/*EXPLAIN*/ DESCRIBE EXTENDED bettercustomers",
      "user": "anonymous",
      "dateUpdated": "2021-03-05 22:34:28.649",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "col_name": "string",
                      "data_type": "string",
                      "comment": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "col_name\tdata_type\tcomment\nid\tint\tnull\ncreated\ttimestamp\tnull\nupdated\ttimestamp\tnull\nfirst_name\tstring\tnull\nlast_name\tstring\tnull\nemail\tstring\tnull\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614186078742_145044323",
      "id": "paragraph_1614186078742_145044323",
      "dateCreated": "2021-02-24 17:01:18.742",
      "dateStarted": "2021-03-05 22:34:28.378",
      "dateFinished": "2021-03-05 22:34:28.414",
      "status": "FINISHED"
    },
    {
      "title": "Get the Table Comments from MySQL Information Schema",
      "text": "%spark\nval schema \u003d spark\n.read\n.format(\"jdbc\")\n.options(Map[String, String](\n    \"url\" -\u003e connectionUrl,\n    \"driver\" -\u003e jdbcDriver,\n    \"query\" -\u003e \n\"\"\"\nselect column_name, data_type, is_nullable, column_comment from information_schema.`columns` \nwhere table_name \u003d \u0027bettercustomers\u0027 \nand table_schema \u003d \u0027default\u0027\n\"\"\",\n    \"user\" -\u003e dbUser,\n    \"password\" -\u003e dbPass\n  )\n)\n.load()\n.toDF(\"name\", \"type\", \"nullable\", \"comment\")\n\nschema\n  .select(\"name\", \"comment\")\n  .where(col(\"comment\").isNotNull)\n  .where(col(\"comment\").notEqual(\"\"))\n  .show(false)\n\n// can turn this into a map and apply it while iterating over the betterCustomers schema.fields -\u003e then we won\u0027t loose the metadata\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-05 22:17:51.684",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-------+---------------------+\n|name   |comment              |\n+-------+---------------------+\n|id     |customer automatic id|\n|created|customer join date   |\n|updated|last record update   |\n+-------+---------------------+\n\n\u001b[1m\u001b[34mschema\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [name: string, type: string ... 2 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d18"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614186138107_513561362",
      "id": "paragraph_1614186138107_513561362",
      "dateCreated": "2021-02-24 17:02:18.107",
      "dateStarted": "2021-03-05 22:17:51.699",
      "dateFinished": "2021-03-05 22:17:53.055",
      "status": "FINISHED"
    },
    {
      "text": "%sql\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-05 22:18:26.772",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1614982706772_1024014974",
      "id": "paragraph_1614982706772_1024014974",
      "dateCreated": "2021-03-05 22:18:26.772",
      "status": "READY"
    }
  ],
  "name": "5-2_MySQLAndSpark",
  "id": "2G1SGWZBW",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}